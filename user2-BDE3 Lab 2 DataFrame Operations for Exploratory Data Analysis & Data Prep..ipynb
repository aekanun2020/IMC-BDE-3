{"cells": [{"cell_type": "code", "execution_count": 1, "id": "ceeb26fa", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 1:>                                                          (0 + 1) / 1]\r"}, {"name": "stdout", "output_type": "stream", "text": "+----+------+------+\n| age|gender|income|\n+----+------+------+\n|  10|      | 10000|\n|  20|Female| 30000|\n|null|  Male| 80000|\n|null|  Male|  5000|\n+----+------+------+\n\n"}, {"name": "stderr", "output_type": "stream", "text": "\r                                                                                \r"}], "source": "from pyspark.sql.functions import *\n\n# Define a dataset.\n\ndf = sc.parallelize([\n    (10, '', 10000), (20, 'Female', 30000), (None, 'Male', 80000), (None, 'Male', 5000)\n]).toDF([\"age\", \"gender\", \"income\"])\n\ndf.show()"}, {"cell_type": "code", "execution_count": 2, "id": "24e4ff05", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "+-------+------------------+------+-----------------+\n|summary|               age|gender|           income|\n+-------+------------------+------+-----------------+\n|  count|                 2|     4|                4|\n|   mean|              15.0|  null|          31250.0|\n| stddev|7.0710678118654755|  null|34247.87098005753|\n|    min|                10|      |             5000|\n|    max|                20|  Male|            80000|\n+-------+------------------+------+-----------------+\n\n"}], "source": "df.describe().show()"}, {"cell_type": "code", "execution_count": 3, "id": "35dee3bc", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+----+------+------+\n| age|gender|income|\n+----+------+------+\n|  10|      | 10000|\n|  20|Female| 30000|\n|15.0|  Male| 80000|\n|15.0|  Male|  5000|\n+----+------+------+\n\n"}], "source": "# Treat Null Value (None) with Average one.\n\navg_age = df.na.drop().agg(avg(\"age\")).collect()[0][0]\n\nsparkf_replaceNull = udf(lambda x: avg_age if x == None else x)\n\nno_null_df = df.withColumn('age', sparkf_replaceNull(col('age')))\n\nno_null_df.show()"}, {"cell_type": "code", "execution_count": 4, "id": "6694bda8", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+----+------+------+-----------+\n| age|gender|income| new_gender|\n+----+------+------+-----------+\n|  10|      | 10000|Male_Assume|\n|  20|Female| 30000|     Female|\n|15.0|  Male| 80000|       Male|\n|15.0|  Male|  5000|       Male|\n+----+------+------+-----------+\n\n"}], "source": "# Treat Missing Value with Defined Values.\n\nfrom pyspark.sql.functions import *\n\ntreat_missing = udf(lambda x: \"Male_Assume\" if x == \"\" else x)\n\nno_missing_df = no_null_df.withColumn('new_gender', treat_missing(no_null_df.gender))\n\nno_missing_df.show()"}, {"cell_type": "code", "execution_count": 5, "id": "19abec1b", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+----+------+------+-----------+\n| age|gender|income| new_gender|\n+----+------+------+-----------+\n|  10|      | 10000|Male_Assume|\n|  20|Female| 30000|     Female|\n|15.0|  Male| 80000|       Male|\n+----+------+------+-----------+\n\n"}], "source": "# Treat Outliner with Remove one.\n\nno_outlier_df = no_missing_df.filter(col('income') >= 10000)\n\nno_outlier_df.show()"}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.12"}}, "nbformat": 4, "nbformat_minor": 5}